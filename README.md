TinyBERT
======== 


For more details about the techniques of TinyBERT, refer to paper:

[TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351)

[origin repo](https://github.com/huawei-noah/Pretrained-Language-Model.git)

Revised
==================================
根据`transformers`修改了数据预处理流程，增加了保存特征的代码，使得程序运行更加顺畅
